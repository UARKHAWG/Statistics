{"cells":[{"cell_type":"markdown","metadata":{"id":"Nd2OOOVXxXS1"},"source":["This study guide should reinforce and provide practice for all of the concepts you have seen in Unit 1 Sprint 1. There are a mix of written questions and coding exercises, both are equally important to prepare you for the sprint challenge as well as to be able to speak on these topics comfortably in interviews and on the job.\n","\n","If you get stuck or are unsure of something remember the 20 minute rule. If that doesn't help, then research a solution with google and stackoverflow. Only once you have exausted these methods should you turn to your track team and mentor - they won't be there on your SC or during an interview. That being said, don't hesitate to ask for help if you truly are stuck.\n","\n","Have fun studying!"]},{"cell_type":"markdown","metadata":{},"source":["What is the difference between null hypothesis and alternative hypothesis? \n","What is the difference between chi square and t-testing? \n","https://www.investopedia.com/terms/h/hypothesistesting.asp#:~:text=Hypothesis%20testing%20is%20an%20act,assumption%20regarding%20a%20population%20parameter.&text=Hypothesis%20testing%20is%20used%20to,from%20a%20data%2Dgenerating%20process.\n","\n","\n","\n","What is the difference between precision and accuracy?\n","https://www.statisticshowto.com/\n","\n","What is correlation? \n","How are features measured against their target variable and from each other? \n","https://en.wikipedia.org/wiki/Correlation"]},{"cell_type":"markdown","metadata":{"id":"fpvInKdXekFi"},"source":["## Questions"]},{"cell_type":"markdown","metadata":{"id":"Q6bS8AhBZ86H"},"source":["Questions\n","\n","When completing this section, try to limit your answers to 2-3 sentences max and use plain english as much as possible. It's very easy to hide incomplete knowledge and undertanding behind fancy or technical words, so imagine you are explaining these things to a non-technical interviewer.\n","\n","1. Describe the Normal Distribution.\n","The normal distribution, also known as the Gaussian distribution, is one of the most important and widely used probability distributions in statistics. It is characterized by several key features:\n","\n","Bell-Shaped Curve: The probability density function (PDF) of a normal distribution forms a symmetric, bell-shaped curve. The highest point on the curve is at the mean (μ), and the curve is symmetrically distributed around this point.\n","\n","Mean, Median, and Mode: In a normal distribution, the mean, median, and mode are all equal and located at the center of the distribution. This point is the highest point on the curve.\n","\n","Constant Standard Deviation: The spread of the data in a normal distribution is determined by the standard deviation (σ). The curve is more peaked and narrow with a smaller standard deviation and wider with a larger standard deviation.\n","\n","Asymptotic Tail: The tails of the normal distribution extend indefinitely in both directions, but the probability of values far from the mean becomes very small as you move away from the center.\n","\n","Empirical Rule: The empirical rule (also known as the 68-95-99.7 rule) applies to normal distributions. It states that approximately:\n","\n","68% of the data falls within one standard deviation of the mean (μ ± σ). 95% falls within two standard deviations (μ ± 2σ). 99.7% falls within three standard deviations (μ ± 3σ). Symmetry: The normal distribution is perfectly symmetric around the mean, with the left and right halves of the curve being mirror images of each other.\n","\n","Parameters: The normal distribution is defined by two parameters: the mean (μ), which sets the center of the distribution, and the standard deviation (σ), which determines the spread or variability of the data.\n","\n","Probability Density Function: The probability density function of the normal distribution is given by the formula:\n","\n","Where:\n","\n","f(x) is the probability density at a specific value x. μ is the mean. σ is the standard deviation. The normal distribution is not only a theoretical concept but also closely approximates the distribution of many natural phenomena, such as heights, weights, IQ scores, and errors in measurements. It is a foundational concept in statistics and is widely used for modeling and analyzing data in various fields, including science, social sciences, and engineering.\n","\n","2. What does the Central Limit Theorem tell us?\n","The Central Limit Theorem (CLT) is a fundamental concept in statistics that tells us the following:\n","\n","Central Limit Theorem Statement: Regardless of the shape of the population distribution, when you take a sufficiently large random sample from that population and calculate the sample means (or other sample statistics), the sampling distribution of those means will approximate a normal distribution, especially as the sample size becomes larger.\n","\n","In other words, the CLT provides important insights into the behavior of sample statistics, such as the sample mean, when multiple random samples are drawn from a population. Here are the key implications and takeaways of the Central Limit Theorem:\n","\n","Normal Approximation: The sampling distribution of the sample mean (or other sample statistics) tends to follow a normal distribution as the sample size increases, regardless of the shape of the population distribution. This holds even for populations that are not normally distributed.\n","\n","Symmetry: The normal distribution is symmetric and bell-shaped, which makes it a convenient distribution for statistical analysis. It simplifies statistical inference and hypothesis testing.\n","\n","Sampling Variability: The CLT explains how the sample mean varies from sample to sample. It tells us that, on average, the sample mean is equal to the population mean. The standard deviation of the sampling distribution (often called the standard error) depends on both the population standard deviation and the sample size.\n","\n","Usefulness in Statistical Inference: The CLT is crucial for statistical inference, including hypothesis testing and confidence interval estimation. It allows us to make inferences about population parameters based on sample statistics.\n","\n","Sample Size Matters: The CLT implies that the larger the sample size, the closer the sampling distribution of the sample mean approximates a normal distribution. Therefore, large sample sizes are often preferred for statistical analysis.\n","\n","Practical Applications: The CLT is widely applied in practice. It is often used to justify the assumption of normality in statistical methods, such as t-tests and confidence intervals, even when dealing with non-normally distributed data.\n","\n","It's important to note that the CLT doesn't specify the exact sample size required for the approximation to be valid, but it suggests that as the sample size gets larger, the approximation becomes better. Researchers and statisticians commonly rely on the rule of thumb that a sample size of around 30 or larger is often sufficient for the CLT to provide a reasonable approximation to a normal distribution. However, the specific requirements can depend on the characteristics of the population distribution and the research context. The symbol 'μ' represents the population mean\n","\n","3. What does the Law of Large Numbers tell us?\n","The Law of Large Numbers (LLN) is a fundamental theorem in probability and statistics that tells us the following:\n","\n","Law of Large Numbers (Strong Version): As the sample size of a random sample from a population increases, the sample mean (or other sample statistics) converges in probability to the population mean. In other words, the larger the sample size, the closer the sample mean gets to the true population mean.\n","\n","Law of Large Numbers (Weak Version): The sample mean converges in probability to the population mean as the sample size increases.\n","\n","Here are the key implications and takeaways of the Law of Large Numbers:\n","\n","Consistency: The LLN implies that as you take larger and larger samples from a population, the sample mean becomes a more consistent and accurate estimator of the population mean. This means that, on average, your sample mean will be very close to the true population mean.\n","\n","Reduction of Random Error: As the sample size increases, the impact of random variation or sampling error on the sample mean decreases. In other words, larger samples provide more stable and reliable estimates.\n","\n","Foundation for Statistical Inference: The LLN is a critical foundation for statistical inference. It justifies the use of sample statistics (such as the sample mean) as estimators for population parameters. This is essential for making inferences and drawing conclusions in statistics.\n","\n","Real-World Application: The LLN is applicable in various real-world scenarios, from quality control in manufacturing to polling in political surveys. It's important to have sufficiently large sample sizes to ensure reliable and accurate estimates of population parameters.\n","\n","It's important to note that the Law of Large Numbers is a fundamental concept in probability and statistics, and it provides the theoretical underpinning for the practical use of sample statistics in statistical analysis and inference. While the LLN does not guarantee that an individual sample will be very close to the population mean, it ensures that, on average, as you collect more and more samples, the sample means will converge to the true population mean.\n","\n","4. What is the t-distribution and how does it relate to the Normal distribution?\n","The t-distribution, also known as Student's t-distribution, is a probability distribution that is used in statistics to model the distribution of sample means or differences in means when the population standard deviation is unknown and has to be estimated from the sample. It is a fundamental tool in hypothesis testing, confidence interval estimation, and statistical inference.\n","\n","Key characteristics of the t-distribution include:\n","\n","Shape: The t-distribution has a bell-shaped curve, much like the normal distribution. However, the shape of the t-distribution is determined by a parameter called the degrees of freedom (df), which affects the thickness of the tails. As the degrees of freedom increase, the t-distribution approaches a normal distribution.\n","\n","Symmetry: Like the normal distribution, the t-distribution is symmetric around its mean.\n","\n","Mean and Median: The mean and median of the t-distribution are both located at zero, which is the center of the distribution.\n","\n","Effect of Degrees of Freedom: The degrees of freedom in the t-distribution are determined by the sample size. As the sample size increases, the t-distribution more closely approximates the standard normal distribution (z-distribution), which has a mean of 0 and a standard deviation of 1.\n","\n","Standardization: To work with the t-distribution, data is often standardized by subtracting the sample mean and dividing by the sample standard deviation. This results in a t-statistic that follows a t-distribution.\n","\n","The relationship between the t-distribution and the normal distribution is as follows:\n","\n","Normal Distribution: The normal distribution is used when you have a large sample size (typically n ≥ 30) or when you know the population standard deviation. In this case, you use the z-distribution, which is a standard normal distribution with a mean of 0 and a standard deviation of 1.\n","\n","t-Distribution: The t-distribution is used when the population standard deviation is unknown and has to be estimated from the sample, or when you have a small sample size (typically n < 30). The shape of the t-distribution depends on the degrees of freedom, which are related to the sample size.\n","\n","Convergence: As the sample size increases, the t-distribution approaches the standard normal distribution. This is consistent with the Central Limit Theorem, which states that the distribution of sample means becomes approximately normal as the sample size becomes larger.\n","\n","In practical terms, the t-distribution is commonly used when working with small sample sizes in hypothesis testing and confidence interval estimation. The choice between the t-distribution and the standard normal distribution (z-distribution) depends on the characteristics of your data and the assumptions you can make about the population standard deviation.\n","\n","5. What is the null hypothesis in a t-test?\n","In a t-test, the null hypothesis (often denoted as H0) is a statement that there is no significant difference or effect, or that a specific parameter (typically the population mean) is equal to a hypothesized value. The null hypothesis is the default assumption that you start with before conducting a t-test.\n","\n","Specifically, in the context of a t-test, the null hypothesis typically takes one of the following forms:\n","\n","One-Sample t-Test:\n","\n","Null Hypothesis: H0: μ = μ0 Here, μ represents the population mean, and μ0 is the hypothesized value of the population mean. The null hypothesis asserts that there is no significant difference between the sample mean and the hypothesized population mean. Two-Sample t-Test:\n","\n","Null Hypothesis for Equal Means: H0: μ1 = μ2 In a two-sample t-test for equal means, the null hypothesis states that there is no significant difference between the means of two independent samples. Null Hypothesis for Paired Samples: H0: μd = 0 In a paired or dependent samples t-test, the null hypothesis asserts that there is no significant difference in the means of paired observations. μd represents the mean of the differences in paired observations. t-Test for the Slope (e.g., Linear Regression):\n","\n","Null Hypothesis: H0: β = 0 In regression analysis, the null hypothesis for the slope (β) asserts that the independent variable has no effect on the dependent variable. The alternative hypothesis (often denoted as Ha) represents the statement you are testing or attempting to provide evidence for. The results of the t-test are used to evaluate whether there is enough evidence to reject the null hypothesis in favor of the alternative hypothesis. Typically, the alternative hypothesis takes one of the following forms:\n","\n","One-Sample t-Test:\n","\n","Alternative Hypothesis: Ha: μ ≠ μ0 (two-tailed) or Ha: μ > μ0 or Ha: μ < μ0 (one-tailed) Two-Sample t-Test:\n","\n","Alternative Hypothesis for Equal Means: Ha: μ1 ≠ μ2 (two-tailed) or Ha: μ1 > μ2 or Ha: μ1 < μ2 (one-tailed) Alternative Hypothesis for Paired Samples: Ha: μd ≠ 0 (two-tailed) or Ha: μd > 0 or Ha: μd < 0 (one-tailed) t-Test for the Slope (e.g., Linear Regression):\n","\n","Alternative Hypothesis: Ha: β ≠ 0 (two-tailed) or Ha: β > 0 or Ha: β < 0 (one-tailed) The goal of the t-test is to assess the strength of evidence from the sample data to either reject the null hypothesis (if there is strong evidence against it) or fail to reject the null hypothesis (if there is insufficient evidence to conclude that the null hypothesis is false).\n","\n","6. What is the alternative hypothesis in a t-test?\n","In a t-test, the alternative hypothesis (often denoted as Ha) represents the statement that you are testing or attempting to provide evidence for. The alternative hypothesis is the opposite or the complement of the null hypothesis (H0), and it typically reflects the direction and nature of the effect or difference you want to detect. The specific form of the alternative hypothesis depends on the type of t-test being conducted. Here are the common forms of the alternative hypothesis in various t-tests:\n","\n","One-Sample t-Test:\n","\n","Null Hypothesis: H0: μ = μ0 Alternative Hypothesis: Ha: μ ≠ μ0 (two-tailed) or Ha: μ > μ0 (right-tailed) or Ha: μ < μ0 (left-tailed) The alternative hypothesis asserts that there is a significant difference between the sample mean (μ) and the hypothesized population mean (μ0). Two-Sample t-Test (for Equal Means):\n","\n","Null Hypothesis: H0: μ1 = μ2 Alternative Hypothesis: Ha: μ1 ≠ μ2 (two-tailed) or Ha: μ1 > μ2 (right-tailed) or Ha: μ1 < μ2 (left-tailed) The alternative hypothesis asserts that there is a significant difference in means between two independent samples. Two-Sample t-Test (for Different Variances):\n","\n","In cases where you want to test whether the variances of two samples are different, the null and alternative hypotheses typically involve variances rather than means. For example: Null Hypothesis: H0: σ1^2 = σ2^2 Alternative Hypothesis: Ha: σ1^2 ≠ σ2^2 (two-tailed) or Ha: σ1^2 > σ2^2 (right-tailed) or Ha: σ1^2 < σ2^2 (left-tailed) Paired Samples t-Test:\n","\n","Null Hypothesis: H0: μd = 0 Alternative Hypothesis: Ha: μd ≠ 0 (two-tailed) or Ha: μd > 0 (right-tailed) or Ha: μd < 0 (left-tailed) The alternative hypothesis asserts that there is a significant difference in means of paired observations (differences, denoted as μd). t-Test for the Slope (e.g., Linear Regression):\n","\n","Null Hypothesis: H0: β = 0 Alternative Hypothesis: Ha: β ≠ 0 (two-tailed) or Ha: β > 0 (right-tailed) or Ha: β < 0 (left-tailed) The alternative hypothesis asserts that the independent variable has a significant effect on the dependent variable (non-zero slope in a regression model). The choice of the specific form of the alternative hypothesis depends on the research question and the nature of the effect you are interested in detecting. The results of the t-test are used to evaluate whether there is enough evidence in the sample data to either reject the null hypothesis in favor of the alternative hypothesis or fail to reject the null hypothesis.\n","\n","\n","7. What is a significance level (also known as an alpha level)?\n","A significance level, often denoted by the Greek letter α (alpha), is a predetermined threshold used in hypothesis testing to determine the level of significance required to reject the null hypothesis. It is a critical component of the hypothesis testing process and helps control the rate of Type I errors, which occur when you incorrectly reject a true null hypothesis.\n","\n","Here's what you need to know about significance levels (alpha levels):\n","\n","Definition: The significance level represents the probability of making a Type I error, which is the chance of incorrectly rejecting a null hypothesis that is, in fact, true. It serves as a pre-established criterion for determining whether the results of a hypothesis test are statistically significant.\n","\n","Common Values: Commonly used significance levels are 0.05 (5%) and 0.01 (1%). However, the choice of α depends on the context of the analysis, the desired level of rigor, and accepted conventions within the field.\n","\n","Decision Rule: In hypothesis testing, you compare the p-value (calculated from the data) to the significance level. The decision rules are as follows:\n","\n","If p-value ≤ α, you reject the null hypothesis. If p-value > α, you fail to reject the null hypothesis. Interpretation: If you set α to 0.05 and your p-value is 0.03, you would reject the null hypothesis at the 0.05 significance level. This means that you have found enough evidence to conclude that the null hypothesis is likely false based on the data.\n","\n","Trade-off: The choice of α involves a trade-off between Type I and Type II errors. A lower α (e.g., 0.01) reduces the chance of making a Type I error but increases the chance of making a Type II error, where you fail to reject a false null hypothesis. Conversely, a higher α (e.g., 0.10) increases the risk of Type I errors but decreases the risk of Type II errors.\n","\n","Context-Dependent: The appropriate significance level depends on the specific research question, the potential consequences of making incorrect decisions, and the field's standards. In some fields, a more conservative α (e.g., 0.01) may be preferred, while in others, a less stringent α (e.g., 0.10) may be acceptable.\n","\n","In summary, a significance level (alpha level) is a critical parameter in hypothesis testing, helping researchers make decisions about whether to reject or fail to reject the null hypothesis based on the strength of the evidence from the data. The choice of α should be carefully considered in the context of the research and its potential impact.\n","\n","\n","8. What is a p-value?\n","A p-value, short for \"probability value\" or \"p-probability,\" is a fundamental concept in statistical hypothesis testing. It quantifies the strength of evidence against the null hypothesis in a hypothesis test. In other words, the p-value helps you assess whether the results of your study are statistically significant or whether they could have occurred by chance.\n","\n","Here's what you need to know about p-values:\n","\n","Definition: The p-value is a probability that measures the likelihood of observing results as extreme as, or more extreme than, those obtained in your study, assuming that the null hypothesis is true. In simple terms, it answers the question, \"What is the probability that the observed data is consistent with the null hypothesis?\"\n","\n","Interpretation:\n","\n","A small p-value (typically less than the chosen significance level, α) suggests that the observed data is unlikely to have occurred by random chance under the null hypothesis. This provides evidence to reject the null hypothesis in favor of the alternative hypothesis. A large p-value suggests that the observed data is consistent with what you would expect if the null hypothesis were true. In this case, you would fail to reject the null hypothesis. Comparing to Significance Level (α): In hypothesis testing, you compare the p-value to a predetermined significance level (α) that you set before conducting the test. The decision rules are:\n","\n","If p-value ≤ α, you reject the null hypothesis. If p-value > α, you fail to reject the null hypothesis. Range: P-values range from 0 to 1, where 0 indicates strong evidence against the null hypothesis, and 1 suggests no evidence against the null hypothesis (i.e., the null hypothesis is plausible based on the data).\n","\n","Not a Probability of Hypotheses: It's important to note that the p-value is not the probability that the null hypothesis is true or false. It measures the probability of the observed data, assuming the null hypothesis is true.\n","\n","Subject to Misinterpretation: Misinterpretation of p-values is common. A small p-value does not prove the null hypothesis false, and a large p-value does not prove the null hypothesis true. P-values should be used in conjunction with other information and the context of the research.\n","\n","No Absolute Cutoff: There is no universally applicable cutoff for what constitutes a small or large p-value. The choice of significance level (α) is context-dependent and should be made based on the research question and the field's standards.\n","\n","In summary, the p-value is a statistical measure that helps you assess the strength of evidence against the null hypothesis. It is a critical tool in hypothesis testing and aids in making informed decisions about whether to reject or fail to reject the null hypothesis based on the observed data.\n","\n","9. How do you use the p-value and alpha level to determine if you should reject or fail to reject the null hypothesis?\n","Set the Significance Level (Alpha Level, α):\n","\n","The significance level, denoted by α, is predetermined and represents the threshold for statistical significance. Common values for α are 0.05 (5%) or 0.01 (1%), but the choice depends on the specific context and the level of rigor you require for your analysis. Conduct Your Hypothesis Test:\n","\n","You conduct your statistical test based on your sample data and the null hypothesis (H0) and alternative hypothesis (Ha) you have formulated. Calculate the p-value:\n","\n","The p-value is a measure of the strength of the evidence against the null hypothesis. It quantifies the probability of obtaining results as extreme as, or more extreme than, what you observed, assuming the null hypothesis is true. A smaller p-value indicates stronger evidence against the null hypothesis. Compare the p-value to α:\n","\n","If the p-value is less than or equal to α, it is considered statistically significant. In this case, you have strong evidence to reject the null hypothesis. This suggests that the observed data is inconsistent with the null hypothesis, and you may favor the alternative hypothesis. Decision Rules:\n","\n","There are two possible decision rules based on the comparison of the p-value and α: If p ≤ α, reject the null hypothesis (H0). If p > α, fail to reject the null hypothesis (H0). Interpretation:\n","\n","If you reject the null hypothesis, you conclude that there is sufficient evidence to suggest that the null hypothesis is likely false. You accept the alternative hypothesis. If you fail to reject the null hypothesis, you conclude that the data do not provide sufficient evidence to suggest that the null hypothesis is false. You do not accept the alternative hypothesis. It's important to note that failing to reject the null hypothesis does not prove the null hypothesis to be true. It means you lack sufficient evidence to reject it based on the current data and analysis. Additionally, the choice of the significance level α should be made in advance and should be based on the context of the study, potential consequences of Type I and Type II errors, and accepted conventions in the field.\n","\n","\n","10. Can you ever prove a null hypothesis is true?\n","No, you cannot prove a null hypothesis to be true in a strict sense. In hypothesis testing, you typically have two hypotheses: the null hypothesis (H0) and the alternative hypothesis (Ha). The null hypothesis represents a statement of no effect or no difference, while the alternative hypothesis represents the statement you are trying to support or demonstrate.\n","\n","The goal of hypothesis testing is not to prove the null hypothesis true but to assess whether the data provides enough evidence to reject the null hypothesis in favor of the alternative hypothesis. You can either:\n","\n","Reject the null hypothesis: This means you have found enough evidence to suggest that the null hypothesis is not true. However, it does not prove the null hypothesis is true; it simply means that the evidence suggests it is likely false.\n","\n","Fail to reject the null hypothesis: This means you do not have enough evidence to conclude that the null hypothesis is false. It does not prove the null hypothesis is true; it only indicates that you don't have sufficient evidence to reject it based on the data at hand.\n","\n","In science and statistics, one can never \"prove\" a null hypothesis with absolute certainty. The best you can do is to gather evidence that suggests the null hypothesis is unlikely to be true or that it is reasonable to assume as a working hypothesis until further evidence suggests otherwise.\n","\n","11. Could you reject the null hypothesis at the alpha = 0.05 level but not at the 0.01 level?\n","Yes, you can reject the null hypothesis at the alpha = 0.05 level but not at the 0.01 level. A lower alpha level (e.g., 0.01) requires stronger evidence to reject the null hypothesis.\n","\n","\n","12. Could you reject the null hypothesis at the alpha = 0.05 level but not at the 0.10 level?\n","Yes, if you reject the null hypothesis at the alpha = 0.05 level, it means that the evidence is strong enough to do so. However, you may not necessarily reject it at the alpha = 0.10 level, as the threshold for significance is higher, allowing for more Type I errors.\n","\n","\n","13. What are the null and alternative hypotheses for a chi-square test?\n","In a chi-square test, the null hypothesis (H0) typically states that there is no association or relationship between the categorical variables being tested. The alternative hypothesis (Ha) posits that there is a significant association or relationship between the variables.\n","\n","\n","14. Explain the difference between a one-sample and two-sample t-test.\n","A one-sample t-test compares the mean of a single sample to a known or hypothesized population mean. In contrast, a two-sample t-test compares the means of two independent samples to determine if they are significantly different from each other.\n","\n","\n","15. Explain the difference betwen a t-test and a chi-square test.\n","A t-test is used to compare means, typically for continuous data, to determine if there is a significant difference between two or more groups. A chi-square test, on the other hand, is used to test the association or independence of categorical variables. It determines whether there is a significant relationship between the variables.\n","\n","\n","16. What do the terms \"frequency\" and \"relative frequency\" mean?\n","\"Frequency\" refers to the number of times a particular value or category occurs in a dataset. \"Relative frequency\" is the proportion of the total observations that a specific value or category represents, often expressed as a percentage.\n","\n","\n","17. What is a statistical distribution?\n","A statistical distribution describes how the values of a variable are distributed or spread in a dataset. It specifies the probability of observing each possible value or range of values, and it is a fundamental concept in statistics.\n","\n","\n","18. What is a joint distribution?\n","A joint distribution is a probability distribution that describes the simultaneous occurrence or relationship between two or more random variables. It provides the probabilities associated with specific combinations of values for the variables.\n","\n","\n","19. What is a conditional distribution?\n","A conditional distribution is a probability distribution that considers one variable while holding another variable constant. It describes the distribution of one variable under specific conditions or values of another variable.\n","\n","\n","20. What is a contingency table?\n","A contingency table, also known as a cross-tabulation or a two-way table, is a table used to display the frequency or counts of observations that fall into specific categories for two or more categorical variables. It is often used to analyze associations or relationships between variables.\n","\n","\n","21. What is a marginal distribution?\n","A marginal distribution represents the distribution of a single variable in a contingency table. It is obtained by summing or averaging the joint distribution across all values of the other variables, effectively reducing it to a single variable's distribution.\n","\n","\n","22. What is the purpose of making a confidence interval?\n","The purpose of making a confidence interval is to provide a range of values within which a population parameter, such as the population mean or proportion, is likely to fall with a certain level of confidence. It quantifies the uncertainty associated with estimating population parameters from sample data.\n","\n","\n","23. What does it mean to be 95% confident?\n","Being 95% confident means that if you were to take many random samples from the same population and construct 95% confidence intervals for a particular parameter, you would expect approximately 95% of those intervals to contain the true population parameter. It reflects the level of confidence in the estimation.\n","\n","\n","24. What is a margin of error?\n","The margin of error is a measure of the uncertainty or variability associated with a sample estimate of a population parameter. It quantifies the range within which the true population parameter is likely to fall and is typically expressed as a range around the point estimate.\n","\n","\n","25. If your confidence level remains the same, will your confidence interval become wider or narrower if you increase your sample size?\n","If you increase your sample size while keeping the confidence level the same, your confidence interval will become narrower. A larger sample size reduces the standard error of the estimate, which results in a more precise estimation of the population parameter.\n","\n","\n","26. Which is more precise: a 90%, 95% or 99% confidence interval?\n","A 99% confidence interval is more precise than a 95% confidence interval, and a 95% confidence interval is more precise than a 90% confidence interval. The level of confidence is inversely related to the width of the confidence interval.\n","\n","\n","27. Which is more accurate: a 90%, 95% or 99% confidence interval?\n","Accuracy in the context of confidence intervals refers to how well the interval estimate approximates the true population parameter. A 95% confidence interval is generally more accurate than a 90% or 99% interval because it balances the trade-off between precision and capturing the true parameter value.\n","\n","\n","28. Can you be 100% confident?\n","n a statistical context, you cannot be 100% confident. Confidence intervals provide a range of values within which a parameter is estimated to lie, and there is always some level of uncertainty or error associated with the estimation.\n","\n","\n","29. What is the law of conditional probability?\n","The law of conditional probability, often referred to as the conditional probability formula, is a fundamental principle in probability theory. It states that the probability of the occurrence of one event given that another event has occurred is equal to the probability of the joint occurrence of both events divided by the probability of the conditioning event. Mathematically, it is expressed as P(A|B) = P(A and B) / P(B), where P(A|B) is the conditional probability of A given B, P(A and B) is the joint probability of A and B, and P(B) is the probability of B.\n","\n","\n","30. Who was Reverend Bayes?  When did he live?\n","Reverend Thomas Bayes was an English mathematician and theologian who lived from around 1701 to 1761. He is best known for his work on probability theory and Bayes' theorem, which he developed posthumously.\n","\n","\n","31. Describe the \"big idea\" of Bayesian statistics.\n","The \"big idea\" of Bayesian statistics is to update our beliefs or knowledge about a parameter or hypothesis as new data becomes available. It involves using prior information (prior beliefs or probabilities) and updating them with observed evidence to arrive at a posterior probability distribution. Bayesian statistics provides a framework for making inferences and decisions that are inherently probabilistic and account for uncertainty.\n","\n","\n","32. Describe two differences between the Bayesian and Frequentist approaches to data analysis.\n","Two key differences between Bayesian and Frequentist approaches to data analysis are: a. Treatment of Uncertainty: Bayesian statistics explicitly quantifies and incorporates prior knowledge and subjective beliefs into the analysis, resulting in a posterior probability distribution that represents the updated belief in light of new data. Frequentist statistics, on the other hand, treats probabilities as frequencies based solely on observed data, without incorporating prior information.\n","\n","b. Interpretation of Probability: Bayesians interpret probability as a measure of uncertainty or degree of belief, which can be subjective and change over time. Frequentists interpret probability as the long-run frequency of events in repeated, hypothetical experiments. This fundamental difference in the interpretation of probability underlies many distinctions between the two approaches.\n","\n","\n","33. Who was Gosset?  When did he live?  What other name is he known by and why did he use a pseudonym?\n","William Sealy Gosset (1876-1937) was an English statistician known for his contributions to the field of statistics. He is more commonly known by his pseudonym, \"Student.\" Gosset worked for the Guinness Brewery in Dublin, Ireland, and he used the pseudonym \"Student\" to publish his statistical work because the brewery did not want its competitors to know that they were using statistical methods to improve the quality of their products. One of his most famous contributions is the development of the Student's t-distribution, which is a fundamental statistical tool in hypothesis testing and confidence interval estimation for small sample sizes.\n","\n","\n","People who will not be on the sprint challenge but you should know as a student of mathematics/computer science:\n","Carl Friedrich Gauss (1777-1855):\n","\n","Gauss was a German mathematician who made significant contributions to various fields, including number theory, statistics, and electromagnetism. He is known for the Gaussian distribution (the normal distribution), the method of least squares, and the Gauss-Markov theorem. Daniel Bernoulli (1700-1782):\n","\n","Daniel Bernoulli was a Swiss mathematician and one of the many notable mathematicians in the Bernoulli family. He made important contributions to probability theory, particularly the Bernoulli distribution, which deals with binary random variables. He is also known for the Bernoulli principle in fluid dynamics. Pierre-Simon Laplace (1749-1827):\n","\n","Laplace was a French mathematician and physicist who contributed to probability theory and celestial mechanics. He developed the Laplace transform, which is widely used in engineering and physics. Laplace's work on probability laid the foundation for Bayesian statistics. Ronald A. Fisher (1890-1962):\n","\n","Fisher was a British statistician and geneticist. He made significant contributions to statistics, including the development of the analysis of variance (ANOVA) and the design of experiments. He is considered one of the founders of modern statistics. Karl Pearson (1857-1936):\n","\n","Karl Pearson was an English statistician and biologist. He is known for developing many statistical techniques, including correlation and the chi-square test. He also founded the journal \"Biometrika\" and contributed to the field of eugenics. Alan Turing (1912-1954):\n","\n","Alan Turing was a British mathematician, logician, and computer scientist. He is best known for his work on the Turing machine, which laid the foundation for modern computer science and artificial intelligence. Turing played a pivotal role in breaking the Enigma code during World War II. Claude Shannon (1916-2001):\n","\n","Claude Shannon, an American mathematician and electrical engineer, is often referred to as the \"father of modern cryptography and information theory.\" His work laid the groundwork for digital circuit design and communication theory. He introduced the concept of the \"bit\" and developed the fundamentals of data transmission and encryption. These individuals made significant contributions to their respective fields and have had a lasting impact on mathematics, statistics, computer science, and related disciplines."]},{"cell_type":"markdown","metadata":{"id":"dUQaIwbceohq"},"source":["## Coding problems"]},{"cell_type":"markdown","metadata":{"id":"4jnYgnFjP6eE"},"source":["Conduct a one-sample t-test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9hFYrmqQlLA"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bCbET5ioQlmQ"},"source":["Conduct a two-sample t-test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwNwPn5nQowi"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"2hq-PhcTQph7"},"source":["Conduct a chi-square test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsWsuYwXRRP3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jVqQYLgxld7M"},"source":["Calculate a frequency and relative frequency"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScovMuwvRdtq"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"uD5cTw9plh9c"},"source":["Calculate a joint distribution and add on the marginal distributions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlazD59ClhXi"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"JyThYBHGlm60"},"source":["Calculate a conditional distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pr_vT8VSmK6J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jsVSa5EXnS_o"},"source":["Calculate a confidence interval for a population mean."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmdHcvXznVec"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gmYQEUVtnVrS"},"source":["Calculate a Bayesian confidence interval."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bs4t6foXnXyC"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Unit 1 Sprint 2 - Study Guide.ipynb","provenance":[{"file_id":"1cYxC-SRCqZgg2hDcohxHdU--8HB-46aO","timestamp":1601892565893},{"file_id":"1lAICG6khpXBJmRXvaMlctWrxm8QhxI5R","timestamp":1601310442388},{"file_id":"10T4m64dmfLsGA91j2v5c5yG7rUzBqB2Q","timestamp":1578879651220},{"file_id":"1SGnO8ZjDtlDUKnPMbzT9QHCO4Fne-fyB","timestamp":1573058832416}]},"kernelspec":{"display_name":"Python 3.8.13","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.13"},"vscode":{"interpreter":{"hash":"38678483988d6aef1b2ae6a777ffa9391823d72da5ae33b583aaacb9f174d266"}}},"nbformat":4,"nbformat_minor":0}
